* Kubernetes Components
  * MASTER Components
    ETCD - ETCD data store stores information regarding nodes,pods, configs, roles, accounts and secrets.
         every information you see when you run kubectl get command that information from ETCD. it stores
         the information in key value format.
   
   kube-apiserver - when run kubectl command that kubectl utility reaches the kube-apiserver kubeapi server
         authenticates the request and validates and then retrieve the information from ETCD.
                    
   kube controller manager - manages various controllers in kubernetes, contineously monitors the status of the pods
        and takes the necessary actions to remediate the situation
                             
   Node controller - contineously monitors the status of the nodes, taking necessary actions to keep the application
        running
        
   Replication controller - contineously monitors the replicasets, desired number of replica sets are running or not,
      if a pod dies it creates another one.
      
  Kube scheduler - it should decide which pod should go to which node
  
* Node Components
  kubelet - An agent that runs on each node in the cluster. It makes sure that containers are running in a Pod.
  
  kube-proxy - kube-proxy is a network proxy that runs on each node in your cluster, kube-proxy maintains network rules on nodes. 
    These network rules allow network communication to your Pods
   
  Container runtime - The container runtime is the software that is responsible for running containers.
         
<== POD ==> 
   A Pod (as in a pod of whales or pea pod) is a group of one or more containers, 
   with shared storage and network resources, and a specification for how to run the containers.
   In terms of Docker concepts, a Pod is similar to a group of Docker containers with shared namespaces and shared filesystem volumes.
      
   YAML file containes:- 
   apiVersion - Which version of the Kubernetes API you're using to create this object
   kind - What kind of object you want to create
   metadata - Data that helps uniquely identify the object, including a name string, UID, and optional namespace
   spec - What state you desire for the object
   
   <== commands ==>
   - kubectl get pods (to check how many pods exist on the system)
   - kubectl run nginx --image=nginx ( to create the pod from nginx image)
   - kubectl describe pod nginx ( to describe and to know more about the pod )
   - kubectl delete pod webapp ( to delete the pod )
   - kubectl run redis --image=redis123 --dry-run=client -o yaml > redis-definition.yaml
   - kubectl label pods redis tier='db'
   - kubectl run custom-nginx --image=nginx --port=8080 (to expose the container port)
   
  <== YAML file to create the pod ==>
  
apiVersion: v1
kind: Pod
metadata:
  name: nginx
  lables:
    app: myapp
    type: front-end
spec:
  containers:
  - name: nginx
    image: nginx:1.14.2
    ports:
    - containerPort: 80
   
<== Replicaset ==> 
 - A ReplicaSet ensures that a specified number of pod replicas are running at any given time. 
 - However, a Deployment is a higher-level concept that manages ReplicaSets and provides declarative updates to Pods along with a lot of other useful features.
 - Therefore, we recommend using Deployments instead of directly using ReplicaSets, unless you require custom update orchestration or don't require updates at all.
 
 <== commands ==>
 - kubectl replace -f replicaset-definition.yaml
 - kubectl scale --replicas=6 -f replicaset-definition.yaml
 - kubectl scale --replicas=6 replicaset myapp-replicaset
 - 
 <== YAML file to create the replicaset ==>
  
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: replicaset-1
spec:
  replicas: 2
  selector:
    matchLabels:
      tier: frontend
  template:
    metadata:
      labels:
        tier: frontend
    spec:
      containers:
      - name: nginx
        image: nginx
         
<== deployment ==>
 - You can define Deployments to create new ReplicaSets, or to remove existing Deployments and adopt all their resources with new Deployments.
 - A new ReplicaSet is created and the Deployment manages moving the Pods from the old ReplicaSet to the new one at a controlled rate.
 - Each new ReplicaSet updates the revision of the Deployment.
         
<== commands ==>
  - kubectl create deployment --image=nginx nginx
  - kubectl create deployment --image=nginx nginx --dry-run=client -o yaml
  - kubectl create deployment --image=nginx nginx --dry-run=client -o yaml > nginx-deployment.yaml
  - kubectl create -f nginx-deployment.yaml
  - kubectl expose deployment nginx --port 80
  - kubectl edit deployment nginx
  - kubectl create deployment --image=nginx nginx --replicas=4 --dry-run=client -o yaml > nginx-deployment.yaml
  - kubectl set image deployment/nginx-deployment nginx=nginx:1.16.1 ( update the nginx Pods to use the nginx:1.16.1 image instead of the nginx:1.14.2 image)
  - kubectl rollout status deployment/nginx-deployment ( To see the rollout status )
  - kubectl rollout undo deployment/nginx-deployment ( undo the current rollout and rollback to the previous revision )
  Note:- you can pause rollouts for that Deployment before you trigger one or more updates
  - kubectl rollout pause deployment/nginx-deployment
  - kubectl rollout history deployment/nginx-deployment
  - kubectl rollout resume deployment/nginx-deployment
  
* Progressing Deployment
 - The Deployment creates a new ReplicaSet
 - The Deployment is scaling up its newest ReplicaSet
 - The Deployment is scaling down its older ReplicaSet
 - New Pods become ready or available
  
<== YAML file to create deployment ==>

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deployment-1
spec:
  replicas: 2
  selector:
    matchLabels:
      name: busybox-pod
  template:
    metadata:
      labels:
        name: busybox-pod
    spec:
      containers:
      - name: busybox-container
        image: busybox888
        command:
        - sh
        - "-c"
        - echo Hello Kubernetes! && sleep 3600
  
<== Services ==?
 - service enables to connect from one group of pods to another group of pods
 - An abstract way to expose an application running on a set of Pods as a network service.
 - Kubernetes gives Pods their own IP addresses and a single DNS name for a set of Pods, and can load-balance across them.
 
 * NodePort
   - Exposes the Service on each Node's IP at a static port (the NodePort).
   - A ClusterIP Service, to which the NodePort Service routes, is automatically created.
   - You'll be able to contact the NodePort Service, from outside the cluster, by requesting <NodeIP>:<NodePort>.
   - If you set the type field to NodePort, the Kubernetes control plane allocates a port from a range (default: 30000-32767).
   
 * ClusterIP
   - Exposes the Service on a cluster-internal IP. Choosing this value makes the Service only reachable from within the cluster.
   
 * External IPs
   - If there are external IPs that route to one or more cluster nodes, Kubernetes Services can be exposed on those externalIPs.
   - Traffic that ingresses into the cluster with the external IP (as destination IP), on the Service port, will be routed to one of the Service endpoints. 
   - 
   
 * LoadBalancer
   -  Exposes the Service externally using a cloud provider's load balancer. 
   -  NodePort and ClusterIP Services, to which the external load balancer routes, are automatically created.
   
<== YAML file for the NodePort ==>
---
apiVersion: v1
kind: Service
metadata:
  name:webapp-service
spec:
  type: NodePort
  ports:
    - targetPort: 8080
      port: 8080
      nodePort: 30080
  selector:
    name: simple-webapp
    
<== YAML file for ClusterIP ==>
apiVersion: v1
kind: Service
metadata:
  name: redis-service
spec:
  type: ClusterIP #optional for ClusterIP
  selector:
    app: redis
  ports:
    - protocol: TCP
      port: 6379
      targetPort: 6379
      
<== YAML file for LoadBalancer ==>
---
apiVersion: v1
kind: Service
metadata:
 name: darwin-service
spec:
 selector:
  app: example
 ports:
  - port: 8765
    targetPort: 9376
    type: loadBalancer
    
<== YAML file for External IPs ==>
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    app: MyApp
  ports:
    - name: http
      protocol: TCP
      port: 80
      targetPort: 9376
  externalIPs:
    - 80.11.12.10
    
 <== commands ==>
 - kubectl create service nodeport <myservicename>
 - kubectl create service clusterip my-svc --clusterip="None" -o yaml --dry-run=client > srv.yaml
 - kubectl create --edit -f srv.yaml
 - kubectl expose pod redis --port=6379 --name redis-service --dry-run=client -o yaml (Create a Service named redis-service of type 
 ClusterIP to expose pod redis on port 6379)
 - kubectl create service clusterip redis --tcp=6379:6379 --dry-run=client -o yaml (This will not use the pods labels as selectors, instead it will assume selectors as app=redis. 
 You cannot pass in selectors as an option. 
 So it does not work very well if your pod has a different label set. 
 So generate the file and modify the selectors before creating the service)
 - kubectl expose pod nginx --type=NodePort --port=80 --name=nginx-service --dry-run-client -o yaml
 (This will automatically use the pod's labels as selectors, but you cannot specify the node port.
 you have to generate a definition file and then add the node port in manually before creating the service with the pod.)
 - kubectl create service nodeport nginx --tcp=80:80 --node-port=30080 --dry-run=client -o yaml
 
* Namespaces
  - Namespaces are a way to divide cluster resources between multiple users
  - In Kubernetes, namespaces provides a mechanism for isolating groups of resources within a single cluster.
  - Names of resources need to be unique within a namespace, but not across namespaces. 
  
  <== commands ==>
  - kubectl get pods --namespace=dev
  - kubectl get pods --all-namespaces
  - kubectl config set-context $(kubectl config current-context) --namespace=dev ( to move the namespace permanently, whatever we create the pods that would be
                                                                                  created on dev namespace)
  - kubectl get ns
  - kubectl run redis --image=redis -n finance 
  
  <== Kubernetes Scheduler ==>
   - A scheduler watches for newly created Pods that have no Node assigned.
   - For every Pod that the scheduler discovers, the scheduler becomes responsible for finding the best Node for that Pod to run on.
   - For every newly created pod or other unscheduled pods, kube-scheduler selects an optimal node for them to run on. 
  
  <== YAML file to schedule pod to the node ==>
  ---
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  -  image: nginx
     name: nginx
  nodeName: node01
  
<== Labels and schedulers ==>
 * Labels
   - Labels are a type of metadata in Kubernetes that take on the form of a key-value pair attached to objects such as pods and services. 
   - Labels are often used to describe identifying aspects of the object, possibly for use by the user at a later stage.
   
* Selectors
  - As their name suggests, label selectors allow you to identify the objects you have tagged with particular labels. 
  <== commands ==>
   - kubectl get pods --selector env=dev ( to filter the pods in dev environment )
   -  kubectl get all --selector env=prod,bu=finance,tier=frontend
   
* taints and tolerations
  - Tolerations are applied to pods, and allow (but do not require) the pods to schedule onto nodes with matching taints.
  - Taints and tolerations work together to ensure that pods are not scheduled onto inappropriate nodes. 
  - One or more taints are applied to a node; this marks that the node should not accept any pods that do not tolerate the taints.
  - pods that do not tolerate the taint are evicted immediately
  
  <== commands ==>
  - kubectl taint nodes node1 key1=value1:NoSchedule
  - kubectl taint nodes node1 key1=value1:NoExecute
  - kubectl taint nodes node1 key2=value2:PreferNoSchedule
  - kubectl taint nodes node01 spray=mortein:NoSchedule
  - kubectl taint nodes controlplane node-role.kubernetes.io/master:NoSchedule- (untaint the node)
 
  
  <== YAML file to tolerate the pod ==>
   ---
apiVersion: v1
kind: Pod
metadata:
  name: bee
spec:
  containers:
  - image: nginx
    name: bee
  tolerations:
  - key: spray
    value: mortein
    effect: NoSchedule
    operator: Equal
    
<== Node Affinity ==>
 - Node affinity is conceptually similar to nodeSelector.
 - allowing you to constrain which nodes your Pod can be scheduled on based on node labels.
 
 <== commands ==>
  - kubectl label nodes node01 key=value
  - kubectl get nodes --show-labels

<== Resources requests and limits ==>
 - Memory requests and limits are associated with Containers, but it is useful to think of a Pod as having a memory request and limit. 
 - The memory request for the Pod is the sum of the memory requests for all the Containers in the Pod.
 - Likewise, the memory limit for the Pod is the sum of the limits of all the Containers in the Pod.
 
 <== YAML file ==>
apiVersion: v1
kind: Pod
metadata:
  name: memory-demo-3
  namespace: mem-example
spec:
  containers:
  - name: memory-demo-3-ctr
    image: polinux/stress
    resources:
      requests:
        memory: "1000Gi"
        cpu: "250m"
      limits:
        memory: "1000Gi"
        cpu: "500m"
    command: ["stress"]
    args: ["--vm", "1", "--vm-bytes", "150M", "--vm-hang", "1"]
    
* DaemonSet
 - A DaemonSet ensures that all (or some) Nodes run a copy of a Pod. As nodes are added to the cluster, Pods are added to them.
 - As nodes are removed from the cluster, those Pods are garbage collected. Deleting a DaemonSet will clean up the Pods it created.
 
* Some typical uses of a DaemonSet are:
  - running a cluster storage daemon on every node
  - running a logs collection daemon on every node
  - running a node monitoring daemon on every node
  
<== commands ==>
 - kubectl get daemonsets
Note:- we can not create the daemonset with the commadn kubectl create daemonset, first have to create the yml file and Kind has to be replaced with DaemonSet

<== YAML file ==>

apiVersion: apps/v1
kind: DaemonSet
metadata:
  labels:
    app: elasticsearch
  name: elasticsearch
  namespace: kube-system
spec:
  selector:
    matchLabels:
      app: elasticsearch
  template:
    metadata:
      labels:
        app: elasticsearch
    spec:
      containers:
      - image: k8s.gcr.io/fluentd-elasticsearch:1.20
        name: fluentd-elasticsearch
        
* Static Pods
 - Static Pods are managed directly by the kubelet daemon on a specific node, without the API server observing them.
 - Unlike Pods that are managed by the control plane (for example, a Deployment); instead, the kubelet watches each static Pod (and restarts it if it fails).
 - The kubelet automatically tries to create a mirror Pod on the Kubernetes API server for each static Pod.
 - This means that the Pods running on a node are visible on the API server, but cannot be controlled from there. 
 - the Pod names will be suffixed with the node hostname with a leading hyphen.
  
 - kubectl top node ( to see the performance metrics )
 - kubectl top pod
 
 * Rolling and Rollout updates
  Rolling - replaces pods running the old version of the application with the new version, one by one, without downtime to the cluster.
  Recreate - Terminate all the pods at a time and replace them with the new version (application faces downtime)
  
 * commands
  - kubectl rollout status <deployment>
  - kubectl rollout history <deployment>
  - kubectl rollout undo <deployment>
    
 * YAML file for Deloyment strategie
 
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
  namespace: default
spec:
  replicas: 4
  selector:
    matchLabels:
      name: webapp
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        name: webapp
    spec:
      containers:
      - image: kodekloud/webapp-color:v2
        name: simple-webapp
        ports:
        - containerPort: 8080
          protocol: TCP
          
* ConfigMap
  A ConfigMap is an API object used to store non-confidential data in key-value pairs.
  Pods can consume ConfigMaps as environment variables, command-line arguments, or as configuration files in a volume
kubectl create configmap webapp --from-literal=APP_COLOUR=darkblue

<== YAML file to pass the env variables to pod ==>

 ---
apiVersion: v1
kind: Pod
metadata:
  labels:
    name: webapp-color
  name: webapp-color
  namespace: default
spec:
  containers:
  - env:
    - name: APP_COLOR
      value: green
    image: kodekloud/webapp-color
    name: webapp-color
    
    
<== YAML to inspect configmap from pod ==>   
---
apiVersion: v1
kind: Pod
metadata:
  labels:
    name: webapp-color
  name: webapp-color
  namespace: default
spec:
  containers:
  - envFrom:
    - configMapRef:
         name: webapp-config-map
    image: kodekloud/webapp-color
    name: webapp-color

* Secrets
  A Secret is an object that contains a small amount of sensitive data such as a password, a token, or a key. 
  Such information might otherwise be put in a Pod specification or in a container image. 
  Kubernetes Secrets are, by default, stored unencrypted in the API server's underlying data store (etcd). 
  Anyone with API access can retrieve or modify a Secret, and so can anyone with access to etcd. 
  Additionally, anyone who is authorized to create a Pod in a namespace can use that access to read any Secret in that namespace;
  
* Commands
  kubectl create secret generic <secret name> --from-literal=<key>=<value>
  kubectl create secret generic <secret name> --from-file=<path-to-file>
  kubectl get secret app-secret -o yaml
  
<== YAML file to create to inject the secret to the pod ==>

---
apiVersion: v1 
kind: Pod 
metadata:
  labels:
    name: webapp-pod
  name: webapp-pod
  namespace: default 
spec:
  containers:
  - image: kodekloud/simple-webapp-mysql
    imagePullPolicy: Always
    name: webapp
    envFrom:
    - secretRef:
        name: db-secret
  
  
              
         
         
         
         
         
         
         
         
         
         
         
         
         
         
         
         
         
         
         
