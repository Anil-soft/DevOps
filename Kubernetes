
* Kubernetes Components
  * MASTER Components
    ETCD - ETCD data store stores information regarding nodes,pods, configs, roles, accounts and secrets.
         every information you see when you run kubectl get command that information from ETCD. it stores
         the information in key value format.
   
   kube-apiserver - when run kubectl command that kubectl utility reaches the kube-apiserver kubeapi server
         authenticates the request and validates and then retrieve the information from ETCD.
                    
   kube controller manager - manages various controllers in kubernetes, contineously monitors the status of the pods
        and takes the necessary actions to remediate the situation
                             
   Node controller - contineously monitors the status of the nodes, taking necessary actions to keep the application
        running
        
   Replication controller - contineously monitors the replicasets, desired number of replica sets are running or not,
      if a pod dies it creates another one.
      
  Kube scheduler - it should decide which pod should go to which node
  
* Node Components
  kubelet - An agent that runs on each node in the cluster. It makes sure that containers are running in a Pod.
  
  kube-proxy - kube-proxy is a network proxy that runs on each node in your cluster, kube-proxy maintains network rules on nodes. 
    These network rules allow network communication to your Pods
   
  Container runtime - The container runtime is the software that is responsible for running containers.
         
<== POD ==> 
   A Pod (as in a pod of whales or pea pod) is a group of one or more containers, 
   with shared storage and network resources, and a specification for how to run the containers.
   In terms of Docker concepts, a Pod is similar to a group of Docker containers with shared namespaces and shared filesystem volumes.
      
   YAML file containes:- 
   apiVersion - Which version of the Kubernetes API you're using to create this object
   kind - What kind of object you want to create
   metadata - Data that helps uniquely identify the object, including a name string, UID, and optional namespace
   spec - What state you desire for the object
   
   <== commands ==>
   - kubectl get pods (to check how many pods exist on the system)
   - kubectl run nginx --image=nginx ( to create the pod from nginx image)
   - kubectl describe pod nginx ( to describe and to know more about the pod )
   - kubectl delete pod webapp ( to delete the pod )
   - kubectl run redis --image=redis123 --dry-run=client -o yaml > redis-definition.yaml
   - kubectl label pods redis tier='db'
   - kubectl run custom-nginx --image=nginx --port=8080 (to expose the container port)
   
  <== YAML file to create the pod ==>
  
apiVersion: v1
kind: Pod
metadata:
  name: nginx
  lables:
    app: myapp
    type: front-end
spec:
  containers:
  - name: nginx
    image: nginx:1.14.2
    ports:
    - containerPort: 80
   
<== Replicaset ==> 
 - A ReplicaSet ensures that a specified number of pod replicas are running at any given time. 
 - However, a Deployment is a higher-level concept that manages ReplicaSets and provides declarative updates to Pods along with a lot of other useful features.
 - Therefore, we recommend using Deployments instead of directly using ReplicaSets, unless you require custom update orchestration or don't require updates at all.
 
 <== commands ==>
 - kubectl replace -f replicaset-definition.yaml
 - kubectl scale --replicas=6 -f replicaset-definition.yaml
 - kubectl scale --replicas=6 replicaset myapp-replicaset
 - 
 <== YAML file to create the replicaset ==>
  
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: replicaset-1
spec:
  replicas: 2
  selector:
    matchLabels:
      tier: frontend
  template:
    metadata:
      labels:
        tier: frontend
    spec:
      containers:
      - name: nginx
        image: nginx
         
<== deployment ==>
 - You can define Deployments to create new ReplicaSets, or to remove existing Deployments and adopt all their resources with new Deployments.
 - A new ReplicaSet is created and the Deployment manages moving the Pods from the old ReplicaSet to the new one at a controlled rate.
 - Each new ReplicaSet updates the revision of the Deployment.
         
<== commands ==>
  - kubectl create deployment --image=nginx nginx
  - kubectl create deployment --image=nginx nginx --dry-run=client -o yaml
  - kubectl create deployment --image=nginx nginx --dry-run=client -o yaml > nginx-deployment.yaml
  - kubectl create -f nginx-deployment.yaml
  - kubectl expose deployment nginx --port 80
  - kubectl edit deployment nginx
  - kubectl create deployment --image=nginx nginx --replicas=4 --dry-run=client -o yaml > nginx-deployment.yaml
  - kubectl set image deployment/nginx-deployment nginx=nginx:1.16.1 ( update the nginx Pods to use the nginx:1.16.1 image instead of the nginx:1.14.2 image)
  - kubectl rollout status deployment/nginx-deployment ( To see the rollout status )
  - kubectl rollout undo deployment/nginx-deployment ( undo the current rollout and rollback to the previous revision )
  Note:- you can pause rollouts for that Deployment before you trigger one or more updates
  - kubectl rollout pause deployment/nginx-deployment
  - kubectl rollout history deployment/nginx-deployment
  - kubectl rollout resume deployment/nginx-deployment
  
* Progressing Deployment
 - The Deployment creates a new ReplicaSet
 - The Deployment is scaling up its newest ReplicaSet
 - The Deployment is scaling down its older ReplicaSet
 - New Pods become ready or available
  
<== YAML file to create deployment ==>

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deployment-1
spec:
  replicas: 2
  selector:
    matchLabels:
      name: busybox-pod
  template:
    metadata:
      labels:
        name: busybox-pod
    spec:
      containers:
      - name: busybox-container
        image: busybox888
        command:
        - sh
        - "-c"
        - echo Hello Kubernetes! && sleep 3600
  
<== Services ==?
 - service enables to connect from one group of pods to another group of pods
 - An abstract way to expose an application running on a set of Pods as a network service.
 - Kubernetes gives Pods their own IP addresses and a single DNS name for a set of Pods, and can load-balance across them.
 
 * NodePort
   - Exposes the Service on each Node's IP at a static port (the NodePort).
   - A ClusterIP Service, to which the NodePort Service routes, is automatically created.
   - You'll be able to contact the NodePort Service, from outside the cluster, by requesting <NodeIP>:<NodePort>.
   - If you set the type field to NodePort, the Kubernetes control plane allocates a port from a range (default: 30000-32767).
   
 * ClusterIP
   - Exposes the Service on a cluster-internal IP. Choosing this value makes the Service only reachable from within the cluster.
   
 * External IPs
   - If there are external IPs that route to one or more cluster nodes, Kubernetes Services can be exposed on those externalIPs.
   - Traffic that ingresses into the cluster with the external IP (as destination IP), on the Service port, will be routed to one of the Service endpoints. 
   - 
   
 * LoadBalancer
   -  Exposes the Service externally using a cloud provider's load balancer. 
   -  NodePort and ClusterIP Services, to which the external load balancer routes, are automatically created.
   
<== YAML file for the NodePort ==>
---
apiVersion: v1
kind: Service
metadata:
  name:webapp-service
spec:
  type: NodePort
  ports:
    - targetPort: 8080
      port: 8080
      nodePort: 30080
  selector:
    name: simple-webapp
    
<== YAML file for ClusterIP ==>
apiVersion: v1
kind: Service
metadata:
  name: redis-service
spec:
  type: ClusterIP #optional for ClusterIP
  selector:
    app: redis
  ports:
    - protocol: TCP
      port: 6379
      targetPort: 6379
      
<== YAML file for LoadBalancer ==>
---
apiVersion: v1
kind: Service
metadata:
 name: darwin-service
spec:
 selector:
  app: example
 ports:
  - port: 8765
    targetPort: 9376
    type: loadBalancer
    
<== YAML file for External IPs ==>
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    app: MyApp
  ports:
    - name: http
      protocol: TCP
      port: 80
      targetPort: 9376
  externalIPs:
    - 80.11.12.10
    
 <== commands ==>
 - kubectl create service nodeport <myservicename>
 - kubectl create service clusterip my-svc --clusterip="None" -o yaml --dry-run=client > srv.yaml
 - kubectl create --edit -f srv.yaml
 - kubectl expose pod redis --port=6379 --name redis-service --dry-run=client -o yaml (Create a Service named redis-service of type 
 ClusterIP to expose pod redis on port 6379)
 - kubectl create service clusterip redis --tcp=6379:6379 --dry-run=client -o yaml (This will not use the pods labels as selectors, instead it will assume selectors as app=redis. 
 You cannot pass in selectors as an option. 
 So it does not work very well if your pod has a different label set. 
 So generate the file and modify the selectors before creating the service)
 - kubectl expose pod nginx --type=NodePort --port=80 --name=nginx-service --dry-run-client -o yaml
 (This will automatically use the pod's labels as selectors, but you cannot specify the node port.
 you have to generate a definition file and then add the node port in manually before creating the service with the pod.)
 - kubectl create service nodeport nginx --tcp=80:80 --node-port=30080 --dry-run=client -o yaml
 
* Namespaces
  - Namespaces are a way to divide cluster resources between multiple users
  - In Kubernetes, namespaces provides a mechanism for isolating groups of resources within a single cluster.
  - Names of resources need to be unique within a namespace, but not across namespaces. 
  
  <== commands ==>
  - kubectl get pods --namespace=dev
  - kubectl get pods --all-namespaces
  - kubectl config set-context $(kubectl config current-context) --namespace=dev ( to move the namespace permanently, whatever we create the pods that would be
                                                                                  created on dev namespace)
  - kubectl get ns
  - kubectl run redis --image=redis -n finance 
  
  <== Kubernetes Scheduler ==>
   - A scheduler watches for newly created Pods that have no Node assigned.
   - For every Pod that the scheduler discovers, the scheduler becomes responsible for finding the best Node for that Pod to run on.
   - For every newly created pod or other unscheduled pods, kube-scheduler selects an optimal node for them to run on. 
  
  <== YAML file to schedule pod to the node ==>
  ---
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  -  image: nginx
     name: nginx
  nodeName: node01
  
<== Labels and schedulers ==>
 * Labels
   - Labels are a type of metadata in Kubernetes that take on the form of a key-value pair attached to objects such as pods and services. 
   - Labels are often used to describe identifying aspects of the object, possibly for use by the user at a later stage.
   
* Selectors
  - As their name suggests, label selectors allow you to identify the objects you have tagged with particular labels. 
  <== commands ==>
   - kubectl get pods --selector env=dev ( to filter the pods in dev environment )
   -  kubectl get all --selector env=prod,bu=finance,tier=frontend
   
* taints and tolerations
  - Tolerations are applied to pods, and allow (but do not require) the pods to schedule onto nodes with matching taints.
  - Taints and tolerations work together to ensure that pods are not scheduled onto inappropriate nodes. 
  - One or more taints are applied to a node; this marks that the node should not accept any pods that do not tolerate the taints.
  - pods that do not tolerate the taint are evicted immediately
  
  <== commands ==>
  - kubectl taint nodes node1 key1=value1:NoSchedule
  - kubectl taint nodes node1 key1=value1:NoExecute
  - kubectl taint nodes node1 key2=value2:PreferNoSchedule
  - kubectl taint nodes node01 spray=mortein:NoSchedule
  - kubectl taint nodes controlplane node-role.kubernetes.io/master:NoSchedule- (untaint the node)
 
  
  <== YAML file to tolerate the pod ==>
   ---
apiVersion: v1
kind: Pod
metadata:
  name: bee
spec:
  containers:
  - image: nginx
    name: bee
  tolerations:
  - key: spray
    value: mortein
    effect: NoSchedule
    operator: Equal
    
<== Node Affinity ==>
 - Node affinity is conceptually similar to nodeSelector.
 - allowing you to constrain which nodes your Pod can be scheduled on based on node labels.
 - requiredDuringSchedulingIgnoredDuringExecution 
 - preferredDuringSchedulingIgnoredDuringExecution 
 
 <== commands ==>
  - kubectl label nodes node01 key=value
  - kubectl get nodes --show-labels

<== Resources requests and limits ==>
 - Memory requests and limits are associated with Containers, but it is useful to think of a Pod as having a memory request and limit. 
 - The memory request for the Pod is the sum of the memory requests for all the Containers in the Pod.
 - Likewise, the memory limit for the Pod is the sum of the limits of all the Containers in the Pod.
 
 <== YAML file ==>
apiVersion: v1
kind: Pod
metadata:
  name: memory-demo-3
  namespace: mem-example
spec:
  containers:
  - name: memory-demo-3-ctr
    image: polinux/stress
    resources:
      requests:
        memory: "1000Gi"
        cpu: "250m"
      limits:
        memory: "1000Gi"
        cpu: "500m"
    command: ["stress"]
    args: ["--vm", "1", "--vm-bytes", "150M", "--vm-hang", "1"]
    
* DaemonSet
 - A DaemonSet ensures that all (or some) Nodes run a copy of a Pod. As nodes are added to the cluster, Pods are added to them.
 - As nodes are removed from the cluster, those Pods are garbage collected. Deleting a DaemonSet will clean up the Pods it created.
 
* Some typical uses of a DaemonSet are:
  - running a cluster storage daemon on every node
  - running a logs collection daemon on every node
  - running a node monitoring daemon on every node
  
<== commands ==>
 - kubectl get daemonsets
Note:- we can not create the daemonset with the commadn kubectl create daemonset, first have to create the yml file and Kind has to be replaced with DaemonSet

<== YAML file ==>

apiVersion: apps/v1
kind: DaemonSet
metadata:
  labels:
    app: elasticsearch
  name: elasticsearch
  namespace: kube-system
spec:
  selector:
    matchLabels:
      app: elasticsearch
  template:
    metadata:
      labels:
        app: elasticsearch
    spec:
      containers:
      - image: k8s.gcr.io/fluentd-elasticsearch:1.20
        name: fluentd-elasticsearch
        
* Static Pods
 - Static Pods are managed directly by the kubelet daemon on a specific node, without the API server observing them.
 - Unlike Pods that are managed by the control plane (for example, a Deployment); instead, the kubelet watches each static Pod (and restarts it if it fails).
 - The kubelet automatically tries to create a mirror Pod on the Kubernetes API server for each static Pod.
 - This means that the Pods running on a node are visible on the API server, but cannot be controlled from there. 
 - Static pods are usually used by software bootstrapping kubernetes itself. For example, 
   kubeadm uses static pods to bringup kubernetes control plane components like api-server,  controller-manager as static pods.
 - the Pod names will be suffixed with the node hostname with a leading hyphen.
  
 - kubectl top node ( to see the performance metrics )
 - kubectl top pod
 
 * Rolling and Rollout updates
  Rolling - replaces pods running the old version of the application with the new version, one by one, without downtime to the cluster.
  Recreate - Terminate all the pods at a time and replace them with the new version (application faces downtime)
  
 * commands
  - kubectl rollout status <deployment>
  - kubectl rollout history <deployment>
  - kubectl rollout undo <deployment>
    
 * YAML file for Deloyment strategie
 
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
  namespace: default
spec:
  replicas: 4
  selector:
    matchLabels:
      name: webapp
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        name: webapp
    spec:
      containers:
      - image: kodekloud/webapp-color:v2
        name: simple-webapp
        ports:
        - containerPort: 8080
          protocol: TCP
          
* ConfigMap
  A ConfigMap is an API object used to store non-confidential data in key-value pairs.
  Pods can consume ConfigMaps as environment variables, command-line arguments, or as configuration files in a volume
kubectl create configmap webapp --from-literal=APP_COLOUR=darkblue

<== YAML file to pass the env variables to pod ==>

 ---
apiVersion: v1
kind: Pod
metadata:
  labels:
    name: webapp-color
  name: webapp-color
  namespace: default
spec:
  containers:
  - env:
    - name: APP_COLOR
      value: green
    image: kodekloud/webapp-color
    name: webapp-color
    
    
<== YAML to inspect configmap from pod ==>   
---
apiVersion: v1
kind: Pod
metadata:
  labels:
    name: webapp-color
  name: webapp-color
  namespace: default
spec:
  containers:
  - envFrom:
    - configMapRef:
         name: webapp-config-map
    image: kodekloud/webapp-color
    name: webapp-color

* Secrets
  A Secret is an object that contains a small amount of sensitive data such as a password, a token, or a key. 
  Such information might otherwise be put in a Pod specification or in a container image. 
  Kubernetes Secrets are, by default, stored unencrypted in the API server's underlying data store (etcd). 
  Anyone with API access can retrieve or modify a Secret, and so can anyone with access to etcd. 
  Additionally, anyone who is authorized to create a Pod in a namespace can use that access to read any Secret in that namespace;
  
* Commands
  kubectl create secret generic <secret name> --from-literal=<key>=<value>
  kubectl create secret generic <secret name> --from-file=<path-to-file>
  kubectl get secret app-secret -o yaml
  
<== YAML file to create to inject the secret to the pod ==>

---
apiVersion: v1 
kind: Pod 
metadata:
  labels:
    name: webapp-pod
  name: webapp-pod
  namespace: default 
spec:
  containers:
  - image: kodekloud/simple-webapp-mysql
    imagePullPolicy: Always
    name: webapp
    envFrom:
    - secretRef:
        name: db-secret

* Multicontainer pods
apiVersion: v1
kind: Pod
metadata:
  name: yellow
spec:
  containers:
  - name: lemon
    image: busybox
    command:
      - sleep
      - "1000"

  - name: gold
    image: redis
  
* Upgrade
  - kubectl drain node01 (take out the node01 from maintenance, pods will be evicted)
  - kubectl uncordon node01 (schedulable the node)
  - kubectl cordon node01 (to unschedulable, future pods will not be scheduled on node01)
  
* Backup the cluster resources
  - kubectl get all --all-namespaces -o yaml > all-deploy-services.yaml
              
* Network Policy
  - If you want to control traffic flow at the IP address or port level (OSI layer 3 or 4), 
    then you might consider using Kubernetes NetworkPolicies for particular applications in your cluster.
  - NetworkPolicies apply to a connection with a pod on one or both ends, and are not relevant to other connections.
  - When defining a pod- or namespace- based NetworkPolicy, you use a selector to specify what traffic is allowed to and from the Pod(s) that match the selector.

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: multi-port-egress
spec:
  podSelector:
    matchLabels:
      role: db
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
            matchLabels:
              name: frontend
      namespaceSelector:
          matchLabels:
            name: prod
    ports:
    - protocol: TCP
      port: 6379x
         
* PersistentVolume and PersistentVolumeClaim
  - A PersistentVolume (PV) is a piece of storage in the cluster that has been provisioned by an administrator or dynamically provisioned using Storage Classes.
  - It is a resource in the cluster just like a node is a cluster resource.
  - PVs are volume plugins like Volumes, but have a lifecycle independent of any individual Pod that uses the PV.
  - A PersistentVolumeClaim (PVC) is a request for storage by a user. It is similar to a Pod. 
  - Pods consume node resources and PVCs consume PV resources.
  - If a user deletes a PVC in active use by a Pod, the PVC is not removed immediately. 
  - PVC removal is postponed until the PVC is no longer actively used by any Pods. 
  - Also, if an admin deletes a PV that is bound to a PVC, the PV is not removed immediately.
  - PV removal is postponed until the PV is no longer bound to a PVC.
  - storage class and PV is available by cluster and PVC is only available on that namespace.
  
  * YAML file for persistentvolume
  apiVersion: v1
  kind: PersistentVolume
  metadata:
    name: pv-vol1
  spec:
    accessModes:
        - ReadWriteOnce
    capacity:
        storage: 1Gi
    hostpath:
       path:/tmp/data
  
* YAML file for persistentvolumeclaims
  apiVersion: v1
  kind: PersistentVolumeClaim
  metadata:
    name: myclaim
  spec:
    accessmodes:
       - ReadWriteOnce
    resources:
       requests:
         storage: 500Mi
         
* Using PVCs on pod

apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
    - name: myfrontend
      image: nginx
      volumeMounts:
      - mountPath: "/var/www/html"
        name: mypd
  volumes:
    - name: mypd
      persistentVolumeClaim:
        claimName: myclaim
        
Note:- ps aux  -> linux command to check the processors running on the host
       ip link -> to check the network interfaces
       ip netns -> to check network namespaces 
 
<== RBAC role based authentication control ==>
    Role-based access control (RBAC) is a method of regulating access to computer or network resources 
    based on the roles of individual users within your organization.
    
Generating private key for John (john.key)
$ openssl genrsa -out john.key 2048

Generating certificate signing request (john.csr)
$ openssl req -new -key john.key -out john.csr -subj "/CN=john/O=finance"

Copy kubernetes ca certificate and key from the master node kmaster
$ scp root@kmaster:/etc/kubernetes/pki/ca.{crt,key} .

Sign the certificate using certificate authority
$ openssl x509 -req -in john.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out john.crt -days 365
    
    * YAML file
      
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: default
  name: pod-reader
rules:
- apiGroups: [""] # "" indicates the core API group
  resources: ["pods"]
  verbs: ["get", "watch", "list"]

  <== kubernetes interview questions and answers ==>
     1. what is taint and tolerations ?
       - Tolerations are applied to pods, and allow (but do not require) the pods to schedule onto nodes with matching taints.
       - Taints and tolerations work together to ensure that pods are not scheduled onto inappropriate nodes. 
       - One or more taints are applied to a node; this marks that the node should not accept any pods that do not tolerate the taints.
       - pods that do not tolerate the taint are evicted immediately
  
       - kubectl taint nodes node1 key1=value1:NoSchedule
       
    2. what is node affinity and pod affinity
       - Node affinity is conceptually similar to nodeSelector.
       - allowing you to constrain which nodes your Pod can be scheduled on based on node labels.
       - requiredDuringSchedulingIgnoredDuringExecution
       - While scheduling workload, when we need to schedule a certain set of pods together, PodAffinity makes sense. Example, a web server and a cache.
       - While scheduling workload, when we need to make sure that a certain set of pods are not scheduled together, PodAntiAffinity makes sense.
       
    3. how to check the logs of a pod or deployment?
       - kubectl logs my-pod 
       - kubectl logs -l name=myLabel (check labels with the label name)
       
    4. what is the deployment strategy in kubernetes?
      - Rolling Update Deployment. The rolling deployment is the default deployment strategy in Kubernetes.
      -  It replaces pods, one by one, of the previous version of our application with pods of the new version without any cluster downtime.
      
    5. what is init containers?
       - A Pod can have multiple containers running apps within it, but it can also have one or more init containers, 
         which are run before the app containers are started.
       - If a Pod's init container fails, the kubelet repeatedly restarts that init container until it succeeds.
       - It contains utilities or setup scripts not present in an app image ( making images light-weight)
       - They always run to completion
       - Init container executes sequentially and each of the init containers must succeed before the next can run.
  <== YAML file ==>      
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
spec:
  containers:
  - name: myapp-container
    image: busybox:1.28
    command: ['sh', '-c', 'echo The app is running! && sleep 3600']
  initContainers:
  - name: init-myservice
    image: busybox:1.28
    command: ['sh', '-c', "until nslookup myservice.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for myservice; sleep 2; done"]
  - name: init-mydb
    image: busybox:1.28
    command: ['sh', '-c', "until nslookup mydb.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for mydb; sleep 2; done"]
    
6. what is statefullset?
   - If you want to use storage volumes to provide persistence for your workload, you can use a StatefulSet as part of the solution. 
  <== YAML file ==>
  
  apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  selector:
    matchLabels:
      app: nginx # has to match .spec.template.metadata.labels
  serviceName: "nginx"
  replicas: 3 # by default is 1
  minReadySeconds: 10 # by default is 0
  template:
    metadata:
      labels:
        app: nginx # has to match .spec.selector.matchLabels
    spec:
      terminationGracePeriodSeconds: 10
      containers:
      - name: nginx
        image: k8s.gcr.io/nginx-slim:0.8
        ports:
        - containerPort: 80
          name: web
        volumeMounts:
        - name: www
          mountPath: /usr/share/nginx/html
  volumeClaimTemplates:
  - metadata:
      name: www
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: "my-storage-class"
      resources:
        requests:
          storage: 1Gi
          
   7. what is ingress in kubernetes?
       - Ingress exposes HTTP and HTTPS routes from outside the cluster to services within the cluster. 
       - Traffic routing is controlled by rules defined on the Ingress resource.
       
      * YAML file
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: minimal-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  ingressClassName: nginx-example
  rules:
  - http:
      paths:
      - path: /testpath
        pathType: Prefix
        backend:
          service:
            name: test
            port:
              number: 80
              
8. what is livenessprobe and readynesspeobe in kubernetes?
   Readiness Probe
  - Readiness probes determine whether or not a container is ready to serve requests. 
  - If the readiness probe returns a failed state, then Kubernetes removes the IP address for the container from the endpoints of all Services.
  
   Livenessprobe
  - Liveness probes determine whether or not an application running in a container is in a healthy state. 
  - If the liveness probe detects an unhealthy state, then Kubernetes kills the container and tries to redeploy it.
  
9. How to connect to the kubernetes pods?
   kubectl exec -it -n <your_namespace> <pod_name> -- bash
   
10. how will you make sure that the database should start first and then application?
    init containers
    
11. Types of storage class used in your project?
    AWS EBS volume
    create an EBS volume and attach it to the node
    step:- 1 create the storage class
    
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: standard
provisioner: kubernetes.io/aws-ebs
parameters:
  type: gp2
reclaimPolicy: Retain
mountOptions:
  - debug
volumeBindingMode: Immediate

    
step:- 2 add volume to the cluster by creating PV
    
kind: PersistentVolume
apiVersion: v1
metadata:
  name: test2
  labels:
    type: amazonEBS
spec:
  capacity:
    storage: 1Gi
accessModes:
  - ReadWriteOnce
awsElasticBlockStore:
  volumeID: vol-0b402cb854e070fad
  fsType: ext4
  
step:-3 create a persistent volume claim

kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: task-pv-claimr
spec:
  storageClassName: standard
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
      
 step:- 4 attach pv to the pod
 
apiVersion: v1
metadata:
name: test-ebs
spec:
  volumes:
    - name: task-pv-storage
      persistentVolumeClaim:
        claimName: task-pv-claimr
  containers:
    - name: nginx
      image: nginx
      volumeMounts:
         - mountPath: “/usr/share/nginx/html”
           name: task-pv-storage
    
12. Update the password in secret without restarting the pod or deployment is it possible ?
    no it is not possible, we have to restart the pod
    
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
  - name: mypod
    image: redis
    volumeMounts:
    - name: foo
      mountPath: "/etc/foo"
      readOnly: true
  volumes:
  - name: foo
    secret:
      secretName: mysecret
      optional: false # default setting; "mysecret" must exist
      
13. what is the reason for pod eviction?
    pod would not be tolerated
    
14. how you will make sure that in rolling update strategy 2 pods are always available?

   RollingUpdate: New pods are added gradually, and old pods are terminated gradually
   Recreate: All old pods are terminated before any new pods are added
   
   maxSurge: The number of pods that can be created above the desired amount of pods during an update
   maxUnavailable: The number of pods that can be unavailable during the update process
   
   replicas: 3
   strategy:
       type: RollingUpdate
       rollingUpdate:
         maxSurge: 1
         maxUnavailable: 0
         
  
  why you are using 3 master node in production?
   In a single master setup, the master node manages the etcd database, API server, controller manager and scheduler, along with the worker nodes. 
   However, if that single master node fails, all the worker node fail as well and entire cluster will be lost. 
   A multi-master setup protects against a wide range of failure modes, from a loss of single worker node to the failure of the master node’s etcd service. 
   By providing redundancy, a multi-master cluster serves a highly available system for your end users.
   
 15. how to check what are the activities performed by the container while creating the pod?
     kubectl describe pod <pod-name>
     
 16. pod conditions?
    PodScheduled: the Pod has been scheduled to a node.
    ContainersReady: all containers in the Pod are ready.
    Initialized: all init containers have completed successfully.
    Ready: the Pod is able to serve requests and should be added to the load balancing pools of all matching Services.
 
 17. how to get the ip of a pod ?
    kubectl get pod -o wide
    
 18. which network plugin you are using?
      calico network
      
  21. pod debugging?
      ImagePullbackOff:- unable to pull the image, because image is not available on DockeHub if the image is in private repository we should log in to DockerHub
      crashloopbackoff:- there is no processor or application is running inside the container, so that starts and keep running always because of restart policy
                         sometimes we may have pod which needs to just come up do it's job and shutdown, in such cases need to mention restartpolicy:Never
      pod in pending state:- make sure that mentioned nodeselector lable at leaset one lable should be labeled in cluster
  
  23. kubectl log nginx -c <container name>  ==> to check the specific container logs in a pod.
  
  22. describe about ingress?
      
      ingress controller :- 
        - Ingress exposes HTTP and HTTPS routes from outside the cluster to services within the cluster. 
        - To process the ingress rules, we must deploy an ingress controller into our cluster, whenever we make any request ingress controller
           inspect that http request direct that request to the correct pod based upon the ingress rules that we defined.
        -  kubectl api-resources | grep <object or service name>  => to list the api versions
       
   <== YAML file for the ingress ==>
   
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: nginx-ingress
spec:
  tls:
    - secretName: nginx-demo-com-tls
      hosts:
        - nginx-demo.com
  rules:
    - host: nginx-demo.com
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: nginx-service
                port:
                  number: 8082
              
23. how to pull image from private docker repo into kubernetes cluster?
    - docker login -u <user-name> -p <password>
    - cat ~/my_password.txt | docker login --username foo --password-stdin 
    - To run the docker login command non-interactively, you can set the --password-stdin flag to provide a password through STDIN
    - Using STDIN prevents the password from ending up in the shell’s history, or log-files.
    - The login process creates or updates a config.json file that holds an authorization token.
       cat ~/.docker/config.json
    - A Kubernetes cluster uses the Secret of kubernetes.io/dockerconfigjson type to authenticate with a container registry to pull a private image.
    - kubectl create secret generic regcred \
    --from-file=.dockerconfigjson=<path/to/.docker/config.json> \
    --type=kubernetes.io/dockerconfigjson
 
    <== YAML file to create secrets ==>
apiVersion: v1
kind: Secret
metadata:
  name: myregistrykey
  namespace: awesomeapps
data:
  .dockerconfigjson: UmVhbGx5IHJlYWxseSByZWVlZWVlZWVlZWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWxsbGxsbGxsbGxsbGxsbGxsbGxsbGxsbGxsbGxsbGx5eXl5eXl5eXl5eXl5eXl5eXl5eSBsbGxsbGxsbGxsbGxsbG9vb29vb29vb29vb29vb29vb29vb29vb29vb25ubm5ubm5ubm5ubm5ubm5ubm5ubm5ubmdnZ2dnZ2dnZ2dnZ2dnZ2dnZ2cgYXV0aCBrZXlzCg==
type: kubernetes.io/dockerconfigjson
 

<== pod manifest file that uses the secret ==>
apiVersion: v1
kind: Pod
metadata:
  name: private-reg
spec:
  containers:
  - name: private-reg-container
    image: <your-private-image>
  imagePullSecrets:
  - name: regcred
 
 How do you secure kubernetes app?
  - 
 
 How to scale your kubernetes application ?
   - The HorizontalPodAutoscaler is implemented as a Kubernetes API resource and a controller.
   - The resource determines the behavior of the controller. 
   - The horizontal pod autoscaling controller, running within the Kubernetes control plane, periodically adjusts the desired scale of its target
     (for example, a Deployment) to match observed metrics such as average CPU utilization, average memory utilization, or any other custom metric you specify.
   - HorizontalPodAutoscaler controls the scale of a Deployment and its ReplicaSet
   
   $ kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10
 
 apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: php-apache
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: php-apache
  minReplicas: 1
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 50
